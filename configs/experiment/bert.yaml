# @package _global_
defaults:
 - override /trainer: default.yaml
 - override /model: null
 - override /dataset: null
 - override /scheduler: null
 - override /optimizer: null
 - override /logger: null
 - override /algorithms: null

seed: 17

logger:
  wandb:
    _target_: composer.loggers.WandBLogger
    name: ${name}
    project: hydra-test
  progress_bar:
    _target_: composer.loggers.ProgressBarLogger

callbacks:
  lr_monitor:
    _target_: composer.callbacks.LRMonitor
  speed_monitor:
    _target_: composer.callbacks.SpeedMonitor
    window_size: 100

model:
  _target_: composer.models.create_bert_mlm
  use_pretrained: false
  tokenizer_name: bert-base-uncased
  pretrained_model_name: bert-base-uncased

# Train the model on the English C4 corpus
dataset:
  train_batch_size: 4096 # Number of training examples to use per update
  eval_batch_size: 2048
  remote: s3://mosaicml-internal-dataset-c4/mds/2/

  train_dataset:
    _target_: composer.datasets.c4.build_streamingc4_dataloader
    remote: ${dataset.remote}
    local: /tmp/mds-cache/mds-c4/
    split: train
    shuffle: true
    tokenizer_name: bert-base-uncased
    max_seq_len: 128
    group_method: truncate
    mlm: true
    mlm_probability: 0.15
    pin_memory: true
    timeout: 120
    prefetch_factor: 2
    persistent_workers: true
    num_workers: 8

  # Periodically evaluate the LanguageCrossEntropy and Masked Accuracy
  # on the validation split of the dataset.
  evaluators:
    pretraining:
      evaluator:
        _target_: composer.core.evaluator.Evaluator
        label: bert_pre_training
        metric_names:
          - LanguageCrossEntropy
          - MaskedAccuracy
      eval_dataset:
        _target_: composer.datasets.c4.build_streamingc4_dataloader
        remote: ${dataset.remote}
        local: /tmp/mds-cache/mds-c4/
        split: val
        shuffle: false
        tokenizer_name: bert-base-uncased
        max_seq_len: 128
        group_method: truncate
        mlm: true
        mlm_probability: 0.15
        pin_memory: true
        timeout: 120
        prefetch_factor: 2
        persistent_workers: true
        num_workers: 8

# Run evaluation after every 1000 training steps

# Use the decoupled AdamW optimizer with learning rate warmup
optimizer:
  _target_: composer.optim.DecoupledAdamW
  lr: 5.0e-4 # Peak learning rate
  betas:
    - 0.9
    - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-5 # Amount of weight decay regularization

scheduler:
  _target_: composer.optim.scheduler.LinearWithWarmupScheduler
  t_warmup: 0.06dur # Point when peak learning rate is reached
  alpha_f: 0.02

trainer:
  max_duration: 286720000sp # Subsample the training data for ~275M samples
  precision: amp # Use mixed-precision training
  grad_clip_norm: -1.0 # Turn off gradient clipping
  grad_accum: auto # Use automatic gradient accumulation to avoid OOMs
  save_folder: bert_checkpoints # The directory to save checkpoints to
  save_interval: 3500ba # Save checkpoints every 3500 batches
  eval_interval: 1000ba
  seed: ${seed}


