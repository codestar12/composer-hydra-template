# @package _global_
defaults:
 - override /trainer: default.yaml
 - override /model: null
 - override /dataset: null
 - override /scheduler: null
 - override /optimizer: null
 - override /logger: wandb.yaml
 - override /algorithms: null

model:
  _target_: composer.models.create_bert_mlm
  use_pretrained: false
  tokenizer_name: bert-base-uncased
  pretrained_model_name: bert-base-uncased

# Train the model on the English C4 corpus
dataset:
  train_batch_size: 4096

  train_dataset:
    _target_: composer.datasets.c4_hparams.StreamingC4Hparams
    remote: s3://allenai-c4/mds/1-gz/
    local: /tmp/mds-cache/mds-c4/
    split: train
    shuffle: true
    tokenizer_name: bert-base-uncased
    max_seq_len: 128
    group_method: truncate
    mlm: true
    mlm_probability: 0.15

  train_dataloader:
    _target_: composer.datasets.dataset_hparams.DataLoaderHparams
    pin_memory: true
    timeout: 0
    prefetch_factor: 2
    persistent_workers: true
    num_workers: 8

  eval_batch_size: 2048
  eval_dataset:
  # Periodically evaluate the LanguageCrossEntropy and Masked Accuracy
  # on the validation split of the dataset.
  evaluators:
    - label: bert_pre_training
      eval_dataset:
        streaming_c4:
          remote: s3://allenai-c4/mds/1-gz/
          local: /tmp/mds-cache/mds-c4/
          split: val
          shuffle: false
          tokenizer_name: bert-base-uncased
          max_seq_len: 128
          group_method: truncate
          mlm: true
          mlm_probability: 0.15
      metric_names:
        - LanguageCrossEntropy
        - MaskedAccuracy

# Run evaluation after every 1000 training steps
eval_interval: 1000ba

# Use the decoupled AdamW optimizer with learning rate warmup
optimizer:
  _target_: composer.optim.DecoupledAdamW
  lr: 5.0e-4 # Peak learning rate
  betas:
    - 0.9
    - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-5 # Amount of weight decay regularization

scheduler:
  _target_: composer.optim.scheduler.LinearWithWarmupScheduler
  t_warmup: 0.06dur # Point when peak learning rate is reached
  alpha_f: 0.02

max_duration: 286720000sp # Subsample the training data for ~275M samples
train_batch_size: 4096 # Number of training examples to use per update
eval_batch_size: 2048

seed: 17

precision: amp # Use mixed-precision training
grad_clip_norm: -1.0 # Turn off gradient clipping
grad_accum: auto # Use automatic gradient accumulation to avoid OOMs

save_folder: bert_checkpoints # The directory to save checkpoints to
save_interval: 3500ba # Save checkpoints every 3500 batches

loggers:
  progress_bar: {} # Add a TQDM progress bar