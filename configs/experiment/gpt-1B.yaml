# @package _global_
defaults:
 - override /trainer: default.yaml
 - override /model: null
 - override /dataset: null
 - override /scheduler: null
 - override /optimizer: null
 - override /logger: null
 - override /callbacks: null
 - override /algorithms: null

callbacks:
  lr_monitor:  
    _target_: composer.callbacks.lr_monitor.LRMonitor
  speed_monitor:
    _target_: composer.callbacks.speed_monitor.SpeedMonitor
    window_size: 10
  memory_monitor:
    _target_: composer.callbacks.MemoryMonitor

logger:
  wandb:
    _target_: composer.loggers.WandBLogger
    name: ${name}
    project: gpt-hydra-test
  progress_bar:
    _target_: composer.loggers.ProgressBarLogger

seed: 17
tokenizer_name: &tokenizer_name gpt2
max_seq_len: &max_seq_len 2048

model:
  _target_: src.models.llm.gpt.ComposerGPT
  _recursive_: false # leave cfg as an OmegaConf object to pass to constructor
  cfg:
    tokenizer_name: *tokenizer_name
    d_model: 2048
    n_heads: 16
    n_layers: 24
    mlp_ratio: 4
    max_seq_len: *max_seq_len
    vocab_size: 50257
    init_std: 0.02
    attn_pdrop: 0.0
    resid_pdrop: 0.0
    emb_pdrop: 0.0
    attn_impl: flash
  device: meta

# Train the model on the English C4 corpus
dataset:
  train_batch_size: 512 # Number of training examples to use per update
  eval_batch_size: 16
  remote: s3://mosaicml-internal-dataset-c4/mds/2/

  train_dataset:
    _target_: src.dataset.c4.data_c4.build_c4_dataloader
    remote: ${dataset.remote}
    local: /tmp/mds-cache/mds-c4/
    split: train
    shuffle: true
    tokenizer_name: *tokenizer_name
    max_seq_len: *max_seq_len
    group_method: concat
    prefetch: 1_000_000
    pin_memory: true
    timeout: 120
    drop_last: true
    prefetch_factor: 2
    persistent_workers: true
    num_workers: 8

  eval_dataset:
    _target_: src.dataset.c4.data_c4.build_c4_dataloader
    remote: ${dataset.remote}
    local: /tmp/mds-cache/mds-c4/
    split: val
    shuffle: false
    prefetch: 1000
    tokenizer_name: *tokenizer_name
    max_seq_len: *max_seq_len
    group_method: truncate
    pin_memory: true
    timeout: 120
    drop_last: false
    prefetch_factor: 2
    persistent_workers: true
    num_workers: 8

# Run evaluation after every 1000 training steps

# Use the decoupled AdamW optimizer with learning rate warmup
optimizer:
  _target_: composer.optim.DecoupledAdamW
  lr: 2.0e-4 # Peak learning rate
  betas:
    - 0.9
    - 0.95
  eps: 1.0e-08
  weight_decay: 0.0 # Amount of weight decay regularization

scheduler:
  _target_: composer.optim.scheduler.CosineAnnealingWithWarmupScheduler
  t_warmup: 100ba # Point when peak learning rate is reached
  alpha_f: 0.1
  t_max: 1dur

trainer:
  max_duration: 24800ba  # ~ 26B tokens
  precision: bf16 
  grad_clip_norm: 1.0 
  grad_accum: auto # Use automatic gradient accumulation to avoid OOMs
  # save_folder: bert_checkpoints # The directory to save checkpoints to
  save_interval: 2000ba # Save checkpoints every 500 batches
  eval_interval: 2000ba
  fsdp_config:
    _convert_: partial # convert Omegaconf to dicstionary to pass arg
    sharding_strategy: FULL_SHARD
    min_params: 2e8
    mixed_precision: DEFAULT
    activation_checkpointing: true
    activation_cpu_offload: false
    verbose: true
  seed: ${seed}

