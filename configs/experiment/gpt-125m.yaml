# @package _global_
defaults:
 - override /trainer: default.yaml
 - override /model: null
 - override /dataset: null
 - override /scheduler: null
 - override /optimizer: null
 - override /logger: null
 - override /callbacks: null
 - override /algorithms: null

callbacks:
  lr_monitor:  
    _target_: composer.callbacks.lr_monitor.LRMonitor
  speed_monitor:
    _target_: composer.callbacks.speed_monitor.SpeedMonitor
    window_size: 1
  memory_monitor:
    _target_: composer.callbacks.MemoryMonitor

logger:
  wandb:
    _target_: composer.loggers.WandbLogger
    name: ${name}
    project: gpt-hydra-test
  progress_bar:
    _target_: composer.loggers.ProgressBarLogger

seed: 17
tokenizer_name: &tokenizer_name gpt2
max_seq_len: &max_seq_len 2048

model:
  _target_: src.models.llm.gpt.ComposerGPT
  _recursive_: false
  cfg:
    tokenizer_name: *tokenizer_name
    d_model: 768
    n_heads: 12
    n_layers: 12
    mlp_ratio: 4
    max_seq_len: *max_seq_len
    vocab_size: 50257
    init_std: 0.02
    attn_pdrop: 0.0
    resid_pdrop: 0.0
    emb_pdrop: 0.0
    attn_impl: flash
  device: meta

# Train the model on the English C4 corpus
dataset:
  train_batch_size: 256 # Number of training examples to use per update
  eval_batch_size: 256
  remote: s3://mosaicml-internal-dataset-c4/mds/2/

  train_dataset:
    _target_: composer.datasets.c4.build_streamingc4_dataloader
    remote: ${dataset.remote}
    local: /tmp/mds-cache/mds-c4/
    split: train
    shuffle: true
    tokenizer_name: *tokenizer_name
    max_seq_len: *max_seq_len
    group_method: concat
    pin_memory: true
    timeout: 120
    prefetch_factor: 2
    persistent_workers: true
    num_workers: 8

  eval_dataset:
    _target_: composer.datasets.c4.build_streamingc4_dataloader
    remote: ${dataset.remote}
    local: /tmp/mds-cache/mds-c4/
    split: val
    shuffle: false
    tokenizer_name: *tokenizer_name
    max_seq_len: *max_seq_len
    group_method: concat
    pin_memory: true
    timeout: 120
    prefetch_factor: 2
    persistent_workers: true
    num_workers: 8

# Run evaluation after every 1000 training steps

# Use the decoupled AdamW optimizer with learning rate warmup
optimizer:
  _target_: composer.optim.DecoupledAdamW
  lr: 6.0e-4 # Peak learning rate
  betas:
    - 0.9
    - 0.95
  eps: 1.0e-08
  weight_decay: 0.0 # Amount of weight decay regularization

scheduler:
  _target_: composer.optim.scheduler.CosineAnnealingWithWarmupScheduler
  t_warmup: 100ba # Point when peak learning rate is reached
  alpha_f: 0.1
  t_max: 1dur

trainer:
  max_duration: 4800ba # Subsample the training data for ~275M samples
  precision: bf16 # Use mixed-precision training
  grad_clip_norm: 1.0 # Turn off gradient clipping
  grad_accum: auto # Use automatic gradient accumulation to avoid OOMs
  # save_folder: bert_checkpoints # The directory to save checkpoints to
  save_interval: 500ba # Save checkpoints every 3500 batches
  eval_interval: 500ba
  fsdp_config:
    _convert_: partial
    sharding_strategy: FULL_SHARD
    min_params: 1e8
    mixed_precision: DEFAULT
    activation_checkpointing: false
    activation_cpu_offload: false
    verbose: true
  seed: ${seed}



loggers:
  progress_bar: {} # Add a TQDM progress bar